{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions play a crucial role in artificial neural networks, allowing them to model complex, non-linear relationships between input and output. Here are some common activation functions and when to use them:\n",
    "\n",
    "1. **Sigmoid Function (Logistic Function):**\n",
    "   - **Equation:** \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\)\n",
    "   - **Range:** (0, 1)\n",
    "   - **Use Case:** Traditionally used in the output layer for binary classification problems. However, its vanishing gradient problem makes it less suitable for hidden layers in deep networks.\n",
    "\n",
    "2. **Hyperbolic Tangent Function (tanh):**\n",
    "   - **Equation:** \\( \\tanh(x) = \\frac{e^{2x} - 1}{e^{2x} + 1} \\)\n",
    "   - **Range:** (-1, 1)\n",
    "   - **Use Case:** Similar to the sigmoid, but with an output range from -1 to 1. It helps mitigate the vanishing gradient problem to some extent compared to the sigmoid.\n",
    "\n",
    "3. **Rectified Linear Unit (ReLU):**\n",
    "   - **Equation:** \\( f(x) = \\max(0, x) \\)\n",
    "   - **Range:** [0, +∞)\n",
    "   - **Use Case:** Most widely used activation function. It introduces non-linearity and helps the model learn complex patterns. However, it can suffer from the \"dying ReLU\" problem (neurons that always output zero and stop learning).\n",
    "\n",
    "4. **Leaky ReLU:**\n",
    "   - **Equation:** \\( f(x) = \\max(\\alpha x, x) \\) where \\( \\alpha \\) is a small positive constant (e.g., 0.01)\n",
    "   - **Range:** (-∞, +∞)\n",
    "   - **Use Case:** Addresses the \"dying ReLU\" problem by allowing a small, non-zero gradient when the input is negative.\n",
    "\n",
    "5. **Parametric ReLU (PReLU):**\n",
    "   - **Equation:** \\( f(x) = \\max(\\alpha x, x) \\) where \\( \\alpha \\) is a learnable parameter\n",
    "   - **Range:** (-∞, +∞)\n",
    "   - **Use Case:** Similar to Leaky ReLU but with the advantage of learning the optimal slope during training.\n",
    "\n",
    "6. **Exponential Linear Unit (ELU):**\n",
    "   - **Equation:** \\( f(x) = \\begin{cases} \\alpha (e^x - 1) & \\text{if } x < 0 \\\\ x & \\text{if } x \\geq 0 \\end{cases} \\) where \\( \\alpha \\) is a hyperparameter (usually set to 1.0)\n",
    "   - **Range:** (-∞, +∞)\n",
    "   - **Use Case:** A smooth alternative to ReLU with some advantages, especially in terms of dealing with the vanishing gradient problem.\n",
    "\n",
    "7. **Softmax Function:**\n",
    "   - **Equation:** \\( \\text{softmax}(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}} \\) for each element \\( x_i \\)\n",
    "   - **Range:** (0, 1) and the sum of all elements is 1\n",
    "   - **Use Case:** Primarily used in the output layer for multi-class classification problems. It converts the raw output into probabilities.\n",
    "\n",
    "The choice of activation function depends on the specific characteristics of your problem and the properties you want the network to have. ReLU and its variants like Leaky ReLU are popular choices for hidden layers in many scenarios due to their simplicity and effectiveness in training deep neural networks. Experimenting with different activation functions is often a part of the model tuning process to find what works best for a given task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
